version: "3.5"

services: 
  backend:
    build:
      context: .
      dockerfile: Dockerfile
    restart: always
    networks:
      - crawler
    container_name: backend
    depends_on:
      - zookeeper
      - kafka
      - db
    env_file: 
      - .env
    environment:
        - TZ=Asia/Taipei
    ports:
      - "8000:8000"
    logging:
      driver: "json-file"
      options:
          max-size: "50m"
          max-file: "3"

  db:
    image: mongo:4.2.16
    container_name: db
    restart: always
    networks:
      - crawler
    env_file: 
      - .env
    volumes: 
      - db_data:/data/db

  zookeeper:
    image: confluentinc/cp-zookeeper:5.4.0
    container_name: zookeeper
    restart: always
    networks:
      - crawler
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes: 
      - zookeper_data:/var/lib/zookeeper/data
      - zookeper_log:/var/lib/zookeeper/log

  kafka:
    image: confluentinc/cp-kafka:6.0.4
    container_name: kafka
    restart: always
    depends_on:
      - zookeeper
    networks:
      - crawler
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: "PLAINTEXT://:29092,PLAINTEXT_HOST://:9092"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    volumes: 
      - kafka_data:/var/lib/kafka/data

volumes: 
  db_data:
  kafka_data: 
  zookeper_data:  
  zookeper_log: 

networks: 
  crawler:
    name: crawler